---
layout: blog-post
title: K-means
jsFile: blogapps/blogapps.nocache.js
excerpt: Simple unsupervised learning - K Means

---

I'd like to show a pretty simple topic and demo. Let's discuss K-means!

Imagine that you have some data points in a 2-d coordinate space, and let's say that you
want to partition the data into K clusters that would give a reasonable representation of
data that's close together. K-means is a very simple algorithm to do just that.

Here are the steps to do it:
Initialize: Assign each data point to a random cluster from 0 to K
Iterate:
1. Calculate the "mean" point for each cluster, using all the data points in that cluster. For 2d-space, this is the average of all X and Y points in the cluster.
2. For each data point, find the closest cluster. Assign the data point to that cluster for the next iteration.

K-means is extensible to other vector spaces than just a 2d coordinate plane. Each datapoint can be a tuple of as many
features as you like. It can be used for clustering documents for search or learning purposes. For that problem,
the features may be the top 50 TF-IDF term ids in the document. There's no reason you couldn't put in other features
like document length or the Page Rank of the document, and weight them as you desire.

The example I want to show, because it's so compelling, is a computer vision application. K-means can be used
to segment images. Imagine that each pixel in the image is a data point with 5 features
1. X-coordinate
2. Y-coordinate
3. Red value
4. Blue value
5. Green value


Great candidates for image segmentation are color or color + distance from some median point. Check out the demo below!
Note that k-means does not necessarily converge to a global optimum. The output depends on the randomly generated
initial configuration. I'd suggest giving it a few tries so you can see what you get.

Let me first give you fair warning: This will run in your browser, and it will take a while. I _strongly_ suggest
using Google Chrome for this demo.
<div id="kmeans">
</div>

I want to give a shoutout to the incredible [scala-gwt](http://scalagwt.github.com/) project. You're running
Scala in your browser when you run the demo! I had to "optimize" the Scala code due to Scala's implementation of
closures to get it to run fast enough for Google Chrome, but it's more or less what's running. If you're interested
in the full thing, please check out the [source-code](https://github.com/jhartman/blog). I'm going to give you a fair warning. I put in a bunch of jars so the repo is huge.
{% highlight scala %}
object KMeans extends Logging {
  val numIters = 10

  def createStartingVectors[T : ClassManifest](data: Array[T], K: Int, random: Random): Array[T] = {
    if (K < data.size) {
      val indexes = scala.collection.mutable.Set.empty[Int]
      while(indexes.size < K) {
        val next = random.nextInt(data.size - 1)
        indexes += next
      }

      indexes.map { idx => data(idx) }.toArray
    } else throw new IllegalArgumentException
  }

  def cluster[T: ClassManifest](data: Iterable[T], clusters: Array[T], distanceFn: (T, T) => Double): Map[T, T] = {
    var i = 0
    val mapping = data.map { dataVector =>
      if (i % 1000 == 0) log.info(i + " points clustered")
      val (dist, closest) = clusters.foldLeft((Double.MaxValue, clusters.head)) { case ((min, c), cluster) =>
        val test = distanceFn(dataVector, cluster)
        if(test < min) (test, cluster) else (min, c)
      }
      i += 1
      (dataVector, closest)
    }
    mapping.toMap
  }

  def apply[T : ClassManifest](data: Array[T], K: Int)
                              (distanceFn: (T, T) => Double)
                              (buildMean: Iterable[T] => T): Map[T, T] = {
    val initial = cluster(data, createStartingVectors(data, K, new Random), distanceFn)

    (0 until numIters).foldLeft(initial) { (clusters, iter) =>
      apply(clusters)(distanceFn)(buildMean)
    }
  }

  def apply[T : ClassManifest](data: Map[T, T])
                              (distanceFn: (T, T) => Double)
                              (buildMean: Iterable[T] => T): Map[T, T] = {

    val clustering = cluster(data.keys, data.values.toArray.toSet.toArray, distanceFn)
    val inversion = invert(clustering)

    inversion.flatMap { case (oldMean, vectors) =>
      val newMean = buildMean(vectors)
      vectors.map { v => (v, newMean) }
    }.toMap

  }
}
{% endhighlight %}
